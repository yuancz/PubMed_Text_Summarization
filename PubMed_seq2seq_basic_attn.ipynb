{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import re\n",
    "from time import sleep\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import pickle\n",
    "import layers\n",
    "import functions\n",
    "import decoder\n",
    "import encoder\n",
    "from dataset import PubMed_Dataset\n",
    "import data_batcher\n",
    "import models\n",
    "import summarizer\n",
    "import data_utils\n",
    "from sys import stdout\n",
    "from vocab import Vocab_Lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meta_dir = os.path.join(os.getcwd(), 'PubMed')\n",
    "log_dir = os.path.join(meta_dir, 'logs')\n",
    "weights_dir = os.path.join(meta_dir, 'weights')\n",
    "params_dir = os.path.join(meta_dir, 'params')\n",
    "data_dir = os.path.join(meta_dir, 'data_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = pickle.load(open(\"data.pickle\", \"rb\"))\n",
    "vocab_lookup = pickle.load(open(os.path.join(meta_dir, \"vocab_lookup_30000.pickle\"), \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_files = []\n",
    "val_files = []\n",
    "test_files = []\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'train' in filename:\n",
    "        train_files.append(os.path.join(data_dir, filename))\n",
    "    elif 'val' in filename:\n",
    "        val_files.append(os.path.join(data_dir, filename))\n",
    "    elif 'test' in filename:\n",
    "        test_files.append(os.path.join(data_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_partition_loader(partition_files):\n",
    "    i = 0\n",
    "    while True:\n",
    "        partition_file = partition_files[i]\n",
    "        i += 1\n",
    "        yield pickle.load(open(partition_file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_partition_loader = data_partition_loader(train_files)\n",
    "val_partition_loader = data_partition_loader(val_files)\n",
    "test_partition_loader = data_partition_loader(test_files)\n",
    "\n",
    "train_data = next(train_partition_loader)\n",
    "val_data = next(val_partition_loader)\n",
    "test_data = next(test_partition_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_batcher = data_batcher.Data_Batcher(train_data, batch_size)\n",
    "val_batcher = data_batcher.Data_Batcher(val_data, batch_size)\n",
    "test_batcher = data_batcher.Data_Batcher(test_data, batch_size)\n",
    "deploy_batcher = data_batcher.Data_Batcher(val_data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d_pad_len = 150\n",
    "s_pad_len = 20\n",
    "embd_dim = 100\n",
    "hidden_size = 512\n",
    "n_layers = 2\n",
    "vocab_size = vocab_lookup.num_words\n",
    "dropout_keep_prob = 0.8\n",
    "bidirectional = False\n",
    "shared_embeddings = True\n",
    "teacher_forcing_ratios = [1.0] \n",
    "teacher_forcing_steps = [1]\n",
    "\n",
    "display_interval = 100\n",
    "val_interval = 1000\n",
    "deploy_interval = 1000\n",
    "save_interval = 10000\n",
    "n_iters = 200000 \n",
    "\n",
    "lr = 0.001\n",
    "DEVICE = 0\n",
    "USE_CUDA = True\n",
    "DEBUG_MODE = False\n",
    "\n",
    "# w2v = pickle.load(open(\"w2v_CNN-Dailymail_100.pickle\", \"rb\"))\n",
    "pretrained_embeddings = None #functions.create_embeddings(vocab_lookup, w2v)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai2-leia/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Trainable Parameters: 8427092\n",
      "<tf.Variable 'embeddings_layer/embeddings:0' shape=(30000, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0' shape=(612, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_0/gru_cell/gates/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0' shape=(612, 512) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_1/gru_cell/gates/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_1/gru_cell/gates/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_1/gru_cell/candidate/kernel:0' shape=(1024, 512) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/multi_rnn_cell/cell_1/gru_cell/candidate/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'attention/attn_W:0' shape=(512, 512) dtype=float32_ref>\n",
      "<tf.Variable 'attention/attn_v:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/proj_W:0' shape=(1024, 100) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_b:0' shape=(30000,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/go_var:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_0/gru_cell/gates/kernel:0' shape=(612, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_0/gru_cell/gates/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_0/gru_cell/candidate/kernel:0' shape=(612, 512) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_0/gru_cell/candidate/bias:0' shape=(512,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_1/gru_cell/gates/kernel:0' shape=(1024, 1024) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_1/gru_cell/gates/bias:0' shape=(1024,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_1/gru_cell/candidate/kernel:0' shape=(1024, 512) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/multi_rnn_cell/cell_1/gru_cell/candidate/bias:0' shape=(512,) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "device_name = '/gpu:{}'.format(DEVICE) if USE_CUDA else '/cpu:{}'.format(DEVICE)\n",
    "\n",
    "if USE_CUDA:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"{}\".format(DEVICE)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "with tf.device(device_name):\n",
    "    net = models.Seq2Seq_Basic_Attn(vocab_size, d_pad_len, s_pad_len, embedding_dim=embd_dim, hidden_size=hidden_size, \n",
    "                                    n_layers=n_layers, bidirectional=bidirectional, pretrained_embeddings=pretrained_embeddings, \n",
    "                                    trainable_embeddings=True, shared_embeddings=shared_embeddings, weight_tying=True,\n",
    "                                    rnn_cell=tf.contrib.rnn.GRUCell)\n",
    "    model = summarizer.Text_Summarization(net, lr=lr, mode='train')\n",
    "    functions.count_params(tf.trainable_variables())\n",
    "    for var in tf.trainable_variables(): print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### params = {key : value for key, value in net.__dict__.items() if not key.startswith('__') and not key.startswith('_')\n",
    "          and not callable(key) and str(type(value)).find('tensorflow') == -1}\n",
    "model_name = net.__class__.__name__\n",
    "\n",
    "if not DEBUG_MODE:\n",
    "    timestamp = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_%H%M%S')\n",
    "    log_file = os.path.join(log_dir, '{}_train_log_{}.txt'.format(model_name, timestamp))\n",
    "    log_description = '0.001 lr, 128 batch size, params: {}\\n'.format(params)\n",
    "    log = open(log_file, 'w')\n",
    "    log.close()\n",
    "    functions.write_to_log(log_description, log_file)\n",
    "    \n",
    "    params_filename = '{}_params_{}.pickle'.format(model_name, timestamp)\n",
    "    with open(os.path.join(params_dir, params_filename), 'wb') as handle:\n",
    "        pickle.dump(params, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    checkpoint_dir = os.path.join(weights_dir, '{}_checkpoints_{}'.format(model_name, timestamp))\n",
    "    os.mkdir(checkpoint_dir)\n",
    "\n",
    "epoch = 0\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    if not DEBUG_MODE:\n",
    "        saver = tf.train.Saver(max_to_keep=100)\n",
    "     \n",
    "    best_val_loss = 10e6\n",
    "    for itr in range(1, n_iters+1):\n",
    "        if itr in teacher_forcing_steps:\n",
    "            teacher_forcing_ratio = teacher_forcing_ratios[teacher_forcing_steps.index(itr)]\n",
    "            \n",
    "        examples, ep = train_batcher.next_batch()\n",
    "        if ep == 1:\n",
    "            try:\n",
    "                train_data = next(train_partition_loader)\n",
    "            except:\n",
    "                epoch += 1\n",
    "                train_partition_loader = data_partition_loader(train_files)\n",
    "                train_data = next(train_partition_loader)\n",
    "            train_batcher = data_batcher.Data_Batcher(train_data, batch_size)\n",
    "            examples, ep = train_batcher.next_batch()\n",
    "            \n",
    "        inputs = [example.source_ids for example in examples] \n",
    "        targets = [example.target_ids for example in examples] \n",
    "        input_lens = [example.source_len for example in examples]\n",
    "        target_lens = [example.target_len for example in examples] \n",
    "\n",
    "        teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        train_loss, train_acc, grad_norm = model.train_step(sess, inputs, targets, input_lens, target_lens, \n",
    "                                                            targets, target_lens, dropout_keep_prob=dropout_keep_prob, \n",
    "                                                            teacher_forcing=teacher_forcing)\n",
    "\n",
    "        if itr % display_interval == 0 or itr == 1:     \n",
    "            log_string = ('[%d, %5d] loss: %.3f, accuracy: %.3f, grad_norm: %.3f' \n",
    "                          % (epoch, itr, train_loss, train_acc, grad_norm))\n",
    "\n",
    "            if not DEBUG_MODE:\n",
    "                functions.write_to_log(log_string, log_file)\n",
    "            print(log_string)\n",
    "\n",
    "        if itr % val_interval == 0:\n",
    "            val_loss, val_acc = 0.0, 0.0\n",
    "            for i in range(int(len(val_batcher.data)/val_batcher.batch_size)):\n",
    "                examples, ep = val_batcher.next_batch()\n",
    "                if ep == 1:\n",
    "                    try:\n",
    "                        val_data = next(val_partition_loader)\n",
    "                    except:\n",
    "                        val_partition_loader = data_partition_loader(val_files)\n",
    "                        val_data = next(val_partition_loader)\n",
    "                    val_batcher = data_batcher.Data_Batcher(val_data, batch_size)\n",
    "                    examples, ep = val_batcher.next_batch()\n",
    "                inputs = [example.source_ids for example in examples]\n",
    "                targets = [example.target_ids for example in examples] \n",
    "                input_lens = [example.source_len for example in examples]\n",
    "                target_lens = [example.target_len for example in examples]\n",
    "                dummy_dec_inputs = np.zeros_like(targets, dtype=int)\n",
    "                dummy_dec_lens = np.zeros_like(target_lens, dtype=int)\n",
    "\n",
    "                val_batch_loss, val_batch_acc = model.val_step(sess, inputs,dummy_dec_inputs, input_lens, dummy_dec_lens, \n",
    "                                                               targets, target_lens)\n",
    "                val_loss += ((val_batch_loss - val_loss)/(i+1))\n",
    "                val_acc += ((val_batch_acc - val_acc)/(i+1))\n",
    "                #val_loss.append(val_batch_loss)\n",
    "                #val_acc.append(val_batch_acc)\n",
    "                if (i+1)*val_batcher.batch_size >= 10000:\n",
    "                    break\n",
    "            #val_loss = np.mean(val_loss)\n",
    "            #val_acc = np.mean(val_acc)\n",
    "            log_string = ('Validation - loss: %.3f, accuracy: %.3f' % (val_loss, val_acc))\n",
    "\n",
    "            if not DEBUG_MODE:\n",
    "                functions.write_to_log(log_string, log_file)\n",
    "            print(log_string)\n",
    "            \n",
    "            if not DEBUG_MODE:\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    weights_prefix = '{}_weights_epoch_{}_itr_{}'.format(model_name, epoch, itr)\n",
    "                    log_msg = \"Weights saved in file: {}\\n\".format(os.path.join(checkpoint_dir, weights_prefix))\n",
    "                    print(log_msg)\n",
    "                    saver.save(sess, os.path.join(checkpoint_dir, weights_prefix))\n",
    "                    functions.write_to_log(log_msg, log_file)\n",
    "                elif itr % save_interval == 0:\n",
    "                    weights_prefix = '{}_weights_epoch_{}_itr_{}'.format(model_name, epoch, itr)\n",
    "                    log_msg = \"Weights saved in file: {}\\n\".format(os.path.join(checkpoint_dir, weights_prefix))\n",
    "                    print(log_msg)\n",
    "                    saver.save(sess, os.path.join(checkpoint_dir, weights_prefix))\n",
    "                    functions.write_to_log(log_msg, log_file)\n",
    "            \n",
    "        if itr % deploy_interval == 0:\n",
    "            examples, _ = deploy_batcher.next_batch()\n",
    "            example = examples[0]\n",
    "            inputs = [example.source_ids]\n",
    "            input_lens = [example.source_len]\n",
    "            dummy_dec_inputs = [np.zeros_like(example.target_ids, dtype=int)]\n",
    "            \n",
    "            predictions = model.deploy(sess, inputs, input_lens, dummy_dec_inputs)\n",
    "            generated_words = [vocab_lookup.convert_id2word(prediction) for prediction in predictions[0]]\n",
    "            \n",
    "            log_string = ('DOCUMENT:\\n{}\\nMODEL:\\n{}\\nGROUND TRUTH:\\n{}'\n",
    "                          .format(example.source_text, \n",
    "                                  ' '.join(generated_words), \n",
    "                                  example.target_text))\n",
    "            if not DEBUG_MODE:\n",
    "                functions.write_to_log(log_string, log_file)\n",
    "            print(log_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
